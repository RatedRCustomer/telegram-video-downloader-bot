from flask import Flask, request, jsonify, Response
import yt_dlp
import gallery_dl
import os
import threading
import time
import uuid
import subprocess
from pathlib import Path
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import queue
from collections import defaultdict
import json
import hashlib
import sqlite3
from prometheus_client import (
    Counter,
    Histogram,
    Gauge,
    generate_latest,
    CONTENT_TYPE_LATEST,
)
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import asyncio
import redis

CACHE_DB = '/downloads/cache.db'
DOWNLOAD_DIR = '/downloads'
COOKIE_FILE = '/cookies/youtube.txt'
MAX_CONCURRENT = 2
MAX_QUEUE = 5

try:
    redis_client = redis.Redis(
        host=os.getenv('REDIS_HOST', 'redis'),
        port=6379,
        db=1,
        decode_responses=True,
        socket_connect_timeout=5
    )
    redis_client.ping()
    print("‚úÖ Redis connected")
except Exception as e:
    print(f"‚ö†Ô∏è Redis not available: {e}")
    redis_client = None

session = requests.Session()
retry = Retry(total=3, backoff_factor=0.3)
adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=20)
session.mount("http://", adapter)
session.mount("https://", adapter)

# Prometheus metrics
downloads_total = Counter(
    "downloads_total", "Total downloads", ["platform", "quality", "format"]
)
downloads_success = Counter(
    "downloads_success", "Successful downloads", ["platform"]
)
downloads_failed = Counter(
    "downloads_failed", "Failed downloads", ["platform", "reason"]
)
cache_hits = Counter("cache_hits_total", "Cache hits", ["platform"])
cache_misses = Counter("cache_misses_total", "Cache misses", ["platform"])
download_duration = Histogram(
    "download_duration_seconds", "Download duration", ["platform"]
)
active_downloads_gauge = Gauge(
    "active_downloads", "Currently active downloads"
)
queue_size_gauge = Gauge("queue_size", "Download queue size")
cache_size_gauge = Gauge("cache_size_mb", "Cache size in MB")

app = Flask(__name__)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è rate limiting —ñ cookies
REDIS_URL = os.getenv("REDIS_URL")
COOKIE_FILE = os.getenv("YTDLP_COOKIE_FILE", "/cookies/youtube.txt")

if REDIS_URL:
    limiter = Limiter(
        key_func=get_remote_address,
        default_limits=["100 per hour", "10 per minute"],
        storage_uri=REDIS_URL,
    )
else:
    # fallback ‚Äì in-memory, —è–∫ –±—É–ª–æ —Ä–∞–Ω—ñ—à–µ
    limiter = Limiter(
        key_func=get_remote_address,
        default_limits=["100 per hour", "10 per minute"],
    )

limiter.init_app(app)

# Download queue
download_queue = queue.Queue(maxsize=5)
downloads = {}
download_stats = defaultdict(int)

# SQLite –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
CACHE_DB = "/downloads/cache.db"


def init_cache_db():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ë–î –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
    conn = sqlite3.connect(CACHE_DB)
    c = conn.cursor()
    c.execute(
        """CREATE TABLE IF NOT EXISTS video_cache
                 (url_hash TEXT PRIMARY KEY,
                  original_url TEXT,
                  file_path TEXT,
                  title TEXT,
                  platform TEXT,
                  quality TEXT,
                  format TEXT,
                  downloaded_at REAL,
                  file_size INTEGER,
                  access_count INTEGER DEFAULT 0,
                  last_accessed REAL)"""
    )
    conn.commit()
    conn.close()


def get_url_hash(url, quality="720p", format="video"):
    """–ì–µ–Ω–µ—Ä—É—î–º–æ hash URL + –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
    key = f"{url}_{quality}_{format}"
    return hashlib.md5(key.encode()).hexdigest()

def check_cache(url, quality, format_type):
    """Check if video is cached (returns dict)"""
    url_hash = hashlib.sha256(f"{url}_{quality}_{format_type}".encode()).hexdigest()
    
    try:
        # 1. Check Redis first (fast!)
        if redis_client:
            cached_path = redis_client.get(f"cache:{url_hash}")
            if cached_path and os.path.exists(cached_path):
                # Update access in background
                redis_client.incr(f"cache:access:{url_hash}")
                
                # Get metadata from SQLite
                conn = sqlite3.connect(CACHE_DB)
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT title, platform FROM video_cache 
                    WHERE url_hash = ?
                """, (url_hash,))
                result = cursor.fetchone()
                conn.close()
                
                title = result[0] if result else "Cached video"
                platform = result[1] if result else "unknown"
                
                return {
                    "cached": True,
                    "file_path": cached_path,
                    "url_hash": url_hash,
                    "title": title,
                    "platform": platform
                }
        
        # 2. Fallback to SQLite
        conn = sqlite3.connect(CACHE_DB)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT file_path, title, platform FROM video_cache 
            WHERE url_hash = ?
        """, (url_hash,))
        result = cursor.fetchone()
        
        # Update access count
        if result:
            cursor.execute("""
                UPDATE video_cache 
                SET access_count = access_count + 1, last_accessed = ?
                WHERE url_hash = ?
            """, (time.time(), url_hash))
            conn.commit()
        
        conn.close()
        
        if result and os.path.exists(result[0]):
            # Store in Redis for next time
            if redis_client:
                redis_client.setex(f"cache:{url_hash}", 3600, result[0])
            
            return {
                "cached": True,
                "file_path": result[0],
                "url_hash": url_hash,
                "title": result[1],
                "platform": result[2]
            }
        
        # Not cached
        return {
            "cached": False,
            "url_hash": url_hash
        }
        
    except Exception as e:
        print(f"Cache check error: {e}")
        return {
            "cached": False,
            "url_hash": url_hash
        }


def save_to_cache(url, file_path, title, platform, quality="720p", format="video"):
    """–ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤—ñ–¥–µ–æ –≤ –∫–µ—à"""
    try:
        url_hash = get_url_hash(url, quality, format)
        file_size = Path(file_path).stat().st_size

        conn = sqlite3.connect(CACHE_DB)
        c = conn.cursor()

        c.execute(
            """INSERT OR REPLACE INTO video_cache 
                     (url_hash, original_url, file_path, title, platform, 
                      quality, format, downloaded_at, file_size, last_accessed)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (
                url_hash,
                url,
                file_path,
                title,
                platform,
                quality,
                format,
                time.time(),
                file_size,
                time.time(),
            ),
        )

        conn.commit()
        conn.close()
        print(f"Saved to cache: {url}")

    except Exception as e:
        print(f"Cache save error: {e}")


class DownloadManager:
    def __init__(self, max_concurrent=2):
        self.max_concurrent = max_concurrent
        self.active_count = 0
        self.lock = threading.Lock()

    def can_start_download(self):
        with self.lock:
            return self.active_count < self.max_concurrent

    def start_download(self):
        with self.lock:
            if self.active_count < self.max_concurrent:
                self.active_count += 1
                return True
            return False

    def finish_download(self):
        with self.lock:
            self.active_count = max(0, self.active_count - 1)


download_manager = DownloadManager()


def get_platform_options(platform, download_id, quality="720p", format="video"):
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏"""

    cookie_opts = {}
    if COOKIE_FILE and Path(COOKIE_FILE).exists():
        cookie_opts["cookiefile"] = COOKIE_FILE

    base_opts = {
        "outtmpl": f"/downloads/{download_id}_%(title)s.%(ext)s",
        "ignoreerrors": False,
        **cookie_opts,
    }

    height_limit = {
        "360p": 360,
        "480p": 480,
        "720p": 720,
        "1080p": 1080,
    }.get(quality, 720)

    # AUDIO-ONLY —Ä–µ–∂–∏–º
    if format == "audio":
        return {
            **base_opts,
            "format": "bestaudio/best",
            "postprocessors": [
                {
                    "key": "FFmpegExtractAudio",
                    "preferredcodec": "mp3",
                    "preferredquality": "192",
                }
            ],
        }

    # INSTAGRAM
    if platform == "instagram":
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/best",
            "merge_output_format": "mp4",
            "writesubtitles": True,
            "subtitleslangs": ["uk", "en", "ru"],
            "embedsubtitles": True,
            "postprocessors": [
                {
                    "key": "FFmpegVideoConvertor",
                    "preferedformat": "mp4",
                },
                {
                    "key": "FFmpegEmbedSubtitle",
                },
            ],
            "postprocessor_args": {
                "videoconvertor": [
                    "-c:v",
                    "libx264",
                    "-preset",
                    "medium",
                    "-crf",
                    "23",
                    "-profile:v",
                    "baseline",
                    "-level",
                    "3.0",
                    "-pix_fmt",
                    "yuv420p",
                    "-bf",
                    "0",
                    "-refs",
                    "1",
                    "-g",
                    "30",
                    "-sc_threshold",
                    "0",
                    "-movflags",
                    "+faststart",
                    "-maxrate",
                    "2M",
                    "-bufsize",
                    "4M",
                    "-c:a",
                    "aac",
                    "-strict",
                    "experimental",
                    "-ar",
                    "44100",
                    "-ac",
                    "2",
                    "-b:a",
                    "128k",
                    "-vf",
                    "scale=trunc(iw/2)*2:trunc(ih/2)*2",
                ]
            },
        }

    # TIKTOK
    elif platform == "tiktok":
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/best",
            "writesubtitles": True,
            "subtitleslangs": ["uk", "en"],
            "embedsubtitles": True,
            "extractor_args": {
                "tiktok": {
                    "api_hostname": "api16-normal-c-useast1a.tiktokv.com"
                }
            },
        }

    # REDDIT
    elif platform == "reddit":
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/bestvideo+bestaudio/best",
            "merge_output_format": "mp4",
            "writesubtitles": True,
            "subtitleslangs": ["uk", "en"],
            "embedsubtitles": True,
        }

    # YOUTUBE
    elif platform == "youtube":
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/bestvideo+bestaudio/best",
            "writesubtitles": True,
            "writeautomaticsub": True,
            "subtitleslangs": ["uk", "en", "ru"],
            "embedsubtitles": True,
            "merge_output_format": "mp4",
        }

    # PINTEREST
    elif platform == "pinterest":
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/bestvideo+bestaudio/best",
        }

    # DEFAULT
    else:
        return {
            **base_opts,
            "format": f"best[height<={height_limit}]/best",
            "writesubtitles": True,
            "subtitleslangs": ["uk", "en"],
            "embedsubtitles": True,
        }


def download_with_ytdlp(url, download_id, platform, quality="720p", format="video"):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ yt-dlp"""
    try:
        ydl_opts = get_platform_options(platform, download_id, quality, format)

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            title = info.get("title", f"{platform}_video_{download_id}")

            downloads[download_id]["title"] = title
            ydl.download([url])

            download_dir = Path("/downloads")
            for file_path in download_dir.glob(f"{download_id}_*"):
                if file_path.is_file() and file_path.stat().st_size > 1000:
                    downloads[download_id]["file_path"] = str(file_path)
                    downloads[download_id]["status"] = "completed"
                    downloads[download_id]["format"] = format

                    save_to_cache(url, str(file_path), title, platform, quality, format)
                    return True

        return False

    except Exception as e:
        print(f"YT-DLP failed for {url}: {e}")
        return False


def download_with_gallery_dl(url, download_id, platform):
    """Backup –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ gallery-dl"""
    try:
        config = {
            "base-directory": "/downloads",
            "filename": f"{download_id}_{{title}}.{{extension}}",
            "skip": False,
        }

        if platform == "instagram":
            config["extractor"] = {"instagram": {"videos": True}}
        elif platform == "tiktok":
            config["extractor"] = {"tiktok": {"api": "mobile", "format": "best"}}
        elif platform == "pinterest":
            config["extractor"] = {"pinterest": {"videos": True, "images": True}}
        elif platform == "reddit":
            config["extractor"] = {"reddit": {"videos": True, "format": "best"}}

        config_path = f"/tmp/gallery_dl_config_{download_id}.json"
        with open(config_path, "w") as f:
            json.dump(config, f)

        cmd = ["gallery-dl", "--config", config_path, url]
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=120
        )

        if result.returncode == 0:
            download_dir = Path("/downloads")
            for file_path in download_dir.glob(f"{download_id}_*"):
                if file_path.is_file() and file_path.stat().st_size > 1000:
                    downloads[download_id]["file_path"] = str(file_path)
                    downloads[download_id]["status"] = "completed"
                    downloads[download_id]["title"] = file_path.stem.replace(
                        f"{download_id}_", ""
                    )
                    os.unlink(config_path)
                    return True

        if os.path.exists(config_path):
            os.unlink(config_path)
        return False

    except Exception as e:
        print(f"Gallery-dl failed for {url}: {e}")
        return False


def managed_download_worker(url, download_id, quality="720p", format="video"):
    """–ö–µ—Ä–æ–≤–∞–Ω–∏–π worker –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º —Ä–µ—Å—É—Ä—Å—ñ–≤"""
    try:
        os.nice(10)
        download_worker(url, download_id, quality, format)
    finally:
        download_manager.finish_download()
        try:
            next_item = download_queue.get_nowait()
            next_id, next_url, next_quality, next_format = next_item
            if downloads[next_id]["status"] == "queued":
                if download_manager.start_download():
                    thread = threading.Thread(
                        target=managed_download_worker,
                        args=(next_url, next_id, next_quality, next_format),
                    )
                    thread.start()
        except queue.Empty:
            pass


def download_worker(url, download_id, quality="720p", format="video"):
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ fallback –ª–æ–≥—ñ–∫–æ—é"""
    start_time = time.time()
    platform = downloads.get(download_id, {}).get(
        "platform", detect_platform(url)
    )

    # Prometheus: —Å—Ç–∞—Ä—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    downloads_total.labels(
        platform=platform, quality=quality, format=format
    ).inc()

    try:
        downloads[download_id]["status"] = "downloading"
        downloads[download_id]["platform"] = platform

        print(
            f"Starting download from {platform}: {url} "
            f"(quality={quality}, format={format})"
        )

        success = download_with_ytdlp(url, download_id, platform, quality, format)

        if (
            not success
            and format == "video"
            and platform in ["tiktok", "reddit", "pinterest", "instagram"]
        ):
            print(f"YT-DLP failed, trying gallery-dl for {platform}")
            success = download_with_gallery_dl(url, download_id, platform)

        if not success:
            downloads[download_id]["status"] = "error"
            downloads[download_id][
                "error"
            ] = f"Both yt-dlp and gallery-dl failed for {platform}"
            downloads_failed.labels(
                platform=platform, reason="download_failed"
            ).inc()
        else:
            downloads_success.labels(platform=platform).inc()
            download_duration.labels(platform=platform).observe(
                time.time() - start_time
            )

    except Exception as e:
        downloads[download_id]["status"] = "error"
        downloads[download_id]["error"] = str(e)
        downloads_failed.labels(platform=platform, reason="exception").inc()
        print(f"Error downloading {url}: {e}")


@app.route("/add", methods=["POST"])
@limiter.limit("5 per minute")
def download_video():
    data = request.json
    url = data.get("url")
    quality = data.get("quality", "720p")
    format = data.get("format", "video")

    if not url:
        return jsonify({"error": "No URL provided"}), 400

    # –ü–ï–†–ï–í–Ü–†–Ø–Ñ–ú–û –ö–ï–®
    cache_result = check_cache(url, quality, format)
    if cache_result["cached"]:
        print(f"‚úÖ CACHE HIT for {url}")
        return (
            jsonify(
                {
                    "status": "completed",
                    "url": url,
                    "cached": True,
                    "file_path": cache_result["file_path"],
                    "title": cache_result["title"],
                    "platform": cache_result["platform"],
                    "format": format,
                    "id": "cached_" + get_url_hash(url, quality, format),
                }
            ),
            200,
        )

    print(f"‚ùå CACHE MISS for {url} - downloading...")

    if download_queue.full():
        return (
            jsonify(
                {
                    "error": "Server busy. Too many downloads in queue.",
                    "queue_full": True,
                }
            ),
            429,
        )

    download_id = str(uuid.uuid4())
    downloads[download_id] = {
        "status": "queued",
        "url": url,
        "file_path": None,
        "error": None,
        "platform": detect_platform(url),
        "quality": quality,
        "format": format,
        "queued_at": time.time(),
    }

    if download_manager.start_download():
        thread = threading.Thread(
            target=managed_download_worker,
            args=(url, download_id, quality, format),
        )
        thread.start()
    else:
        try:
            download_queue.put((download_id, url, quality, format), timeout=1)
        except queue.Full:
            return jsonify({"error": "Queue full"}), 429

    return jsonify({"status": "queued", "url": url, "id": download_id}), 200


def detect_platform(url):
    url_lower = url.lower()
    if "tiktok.com" in url_lower or "vm.tiktok.com" in url_lower:
        return "tiktok"
    elif "instagram.com" in url_lower:
        return "instagram"
    elif "youtube.com" in url_lower or "youtu.be" in url_lower:
        return "youtube"
    elif "twitter.com" in url_lower or "x.com" in url_lower:
        return "twitter"
    elif "facebook.com" in url_lower or "fb.watch" in url_lower:
        return "facebook"
    elif "reddit.com" in url_lower:
        return "reddit"
    elif "pinterest.com" in url_lower or "pin.it" in url_lower:
        return "pinterest"
    else:
        return "unknown"


@app.route("/status/<download_id>", methods=["GET"])
def get_status(download_id):
    if download_id.startswith("cached_"):
        return jsonify({"status": "completed", "cached": True}), 200

    if download_id in downloads:
        return jsonify(downloads[download_id]), 200
    else:
        return jsonify({"error": "Download not found"}), 404


@app.route("/cache/stats", methods=["GET"])
def cache_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É"""
    try:
        conn = sqlite3.connect(CACHE_DB)
        c = conn.cursor()

        c.execute("SELECT COUNT(*), SUM(file_size), SUM(access_count) FROM video_cache")
        result = c.fetchone()
        count = result[0] or 0
        total_size = result[1] or 0
        total_accesses = result[2] or 0

        c.execute(
            """SELECT platform, COUNT(*), SUM(file_size) 
                     FROM video_cache GROUP BY platform"""
        )
        platform_stats = c.fetchall()

        conn.close()

        return (
            jsonify(
                {
                    "total_cached": count,
                    "total_size_mb": total_size / 1024 / 1024,
                    "total_accesses": total_accesses,
                    "cache_hits_saved": total_accesses,
                    "by_platform": {
                        p: {"count": c, "size_mb": s / 1024 / 1024}
                        for p, c, s in platform_stats
                    },
                }
            ),
            200,
        )

    except Exception as e:
        print(f"Cache stats error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/health", methods=["GET"])
def health_check():
    return jsonify(
        {
            "status": "ok",
            "version": "v3.0-cached",
            "yt_dlp_version": yt_dlp.version.__version__,
            "system": {
                "active_downloads": download_manager.active_count,
                "max_concurrent": download_manager.max_concurrent,
                "queue_size": download_queue.qsize(),
                "max_queue_size": download_queue.maxsize,
                "total_downloads": len(downloads),
            },
            "features": [
                "Video caching (no re-downloads)",
                "Audio extraction (MP3)",
                "Quality selection (360p-1080p)",
                "Ukrainian subtitles",
                "Mobile-optimized encoding",
                "7 platforms support",
            ],
            "supported_platforms": [
                "YouTube",
                "Instagram",
                "Twitter/X",
                "Facebook",
                "TikTok",
                "Reddit",
                "Pinterest",
            ],
        }
    ), 200


@app.route("/metrics", methods=["GET"])
def metrics():
    """Prometheus metrics endpoint"""
    try:
        active_downloads_gauge.set(download_manager.active_count)
        queue_size_gauge.set(download_queue.qsize())

        conn = sqlite3.connect(CACHE_DB)
        c = conn.cursor()
        c.execute("SELECT SUM(file_size) FROM video_cache")
        result = c.fetchone()
        total_size = (result[0] or 0) / 1024 / 1024
        cache_size_gauge.set(total_size)
        conn.close()

        return Response(generate_latest(), mimetype=CONTENT_TYPE_LATEST)
    except Exception as e:
        print(f"Metrics error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/", methods=["GET"])
def health():
    return jsonify({"status": "ok", "version": "v3.0"}), 200


def cleanup_old_cache():
    """–û—á–∏—â–∞—î–º–æ —Å—Ç–∞—Ä–∏–π –∫–µ—à"""
    while True:
        time.sleep(3600)
        try:
            conn = sqlite3.connect(CACHE_DB)
            c = conn.cursor()

            week_ago = time.time() - (7 * 24 * 60 * 60)
            c.execute(
                """SELECT file_path FROM video_cache 
                        WHERE downloaded_at < ? AND access_count < 2""",
                (week_ago,),
            )

            old_files = c.fetchall()
            for (file_path,) in old_files:
                try:
                    Path(file_path).unlink(missing_ok=True)
                except Exception:
                    pass

            c.execute(
                """DELETE FROM video_cache 
                        WHERE downloaded_at < ? AND access_count < 2""",
                (week_ago,),
            )

            conn.commit()
            conn.close()
            print(f"üßπ Cache cleanup: removed {len(old_files)} old files")

        except Exception as e:
            print(f"Cache cleanup error: {e}")


def cleanup_old_downloads():
    """–û—á–∏—â–∞—î–º–æ in-memory downloads"""
    while True:
        time.sleep(600)
        current_time = time.time()
        to_remove = []

        for download_id, data in list(downloads.items()):
            if current_time - data.get("queued_at", current_time) > 3600:
                to_remove.append(download_id)

        for download_id in to_remove:
            downloads.pop(download_id, None)


# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
init_cache_db()
cleanup_cache_thread = threading.Thread(
    target=cleanup_old_cache, daemon=True
)
cleanup_cache_thread.start()
cleanup_thread = threading.Thread(
    target=cleanup_old_downloads, daemon=True
)
cleanup_thread.start()

if __name__ == "__main__":
    print(f"üöÄ Starting yt-dlp API v3.0 with yt-dlp {yt_dlp.version.__version__}")
    print("‚úÖ Video caching enabled")
    print("‚úÖ Audio extraction enabled")
    print("‚úÖ Quality selection enabled")
    print("‚úÖ Ukrainian subtitles enabled")
    # –£ –ø—Ä–æ–¥—ñ –±—É–¥–µ–º–æ –∑–∞–ø—É—Å–∫–∞—Ç–∏ —á–µ—Ä–µ–∑ gunicorn, –∞–ª–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É –Ω–µ—Ö–∞–π –ª–∏—à–∞—î—Ç—å—Å—è:
    app.run(host="0.0.0.0", port=8081, debug=False, threaded=True)
